---
title: "Project 2"
output: html_document
---

```{r}
library(readr)
library(glmnet)      
library(randomForest) 
library(ggplot2) 
library(tidyverse) 
```

# 1. Data from the US National Health and Nutrition Examination Study (NHANES) has been processed, cleaned, and randomly subset, the CSV file is available on canvas. The example includes N=5,000 individuals and 13 variables

```{r}
read.csv("NHANES_diabetes_clean_sub.csv")
```

## a. Estimate a predictor for diabetes (1=yes, 0 = no) given the other health and demographic variables (outcome is the probability of diabetes). Describe your approach to selecting features, tuning parameters, and the evaluation method for model selection. You can use any of the methods from the course.

```{r}
df2 <- read_csv("NHANES_diabetes_clean_sub.csv")

# Create X and Y
Y_glmnet <- df2$diabetes                   
X <- df2 %>% select(-diabetes)    

# Convert to numeric matrix
X_glmnet <- model.matrix(~ ., data = X)[, -1]  

# Check structures
str(Y_glmnet)
str(X_glmnet)

# Fit Elastic Net with 10-fold CV
set.seed(20)
fit_en_cv <- cv.glmnet(X_glmnet, Y_glmnet, 
                       alpha = 0.5, 
                       nfolds = 10, 
                       family = "binomial",  
                       type.measure = "class") 
plot(fit_en_cv)

# Print lambda values
cat("Elastic Net - Min lambda:", log(fit_en_cv$lambda.min), "\n")
cat("Elastic Net - 1SE lambda:", log(fit_en_cv$lambda.1se), "\n")

fit_en_cv$lambda.min
fit_en_cv$lambda.1se
```

To predict the probability of diabetes (1 = yes, 0 = no) using health and demographic variables, I used Elastic Net logistic regression, a regularized regression method that balances the strengths of both Lasso (L1) and Ridge (L2) penalties. It combines the strengths of both Ridge and Lasso; it performs feature selection like Lasso (removes unimportant variables) but handles correlated variables better than Lasso like Ridge. In terms of feature selection, Elastic Net performs automatic feature selection through cross-validation; it selects an optimal lambda value that controls the overall penalty strength. For parameter tuning, lambda controls the strength of the regularization. A higher lambda penalizes model complexity more strongly, shrinking coefficients toward zero and potentially removing less informative predictors. Alpha was set to 0.5 to define the model as Elastic Net, which balances the effects of Lasso (L1) and Ridge (L2) regularization by giving equal weights. For evaluation method, I used 10-fold cross-validation. This approach involves dividing the data into 10 equal parts, training the model on 9 parts, and testing it on the remaining part—repeating this process 10 times so that each fold serves as the test set once. The average classification error across folds is then computed for each candidate value of the regularization parameter (lambda). I selected the value of lambda that minimized this error (lambda.min) and also considered lambda.1se, which provides a simpler model within one standard error of the minimum error. This cross-validation strategy helps ensure that the selected model generalizes well to new data and avoids overfitting.

The lambda that minimized misclassification error was 0.0213 (log lambda ≈ -3.85), as shown by the lowest point on the cross-validation curve. A more regularized model with lambda = 0.1728 (log ≈ -1.76) falls within one standard error of the minimum and would yield a simpler model with fewer predictors while maintaining similar error.

## b. Using a resampling method, estimate the ROC curve for the predictor and report the area under the ROC curve.

```{r}

library(glmnet)
library(plotROC)
library(tidyverse)

# Prepare data
X_full <- model.matrix(~ . - diabetes, data = df2)[, -1]
Y_glmnet <- df2$diabetes
set.seed(20)
N <- nrow(df2)
V <- 10
folds <- split(1:N, rep(1:V, length.out = N))

# Stack the CV predictions (Method 2)
cvPred <- rep(NA, length = N)

# Loop over folds
for (v in 1:V) {
  tmp_train_idx <- setdiff(1:N, folds[[v]])
  tmp_test_idx  <- folds[[v]]
  X_train <- X_full[tmp_train_idx, ]
  Y_train <- Y_glmnet[tmp_train_idx]
  X_test  <- X_full[tmp_test_idx, ]
  fit_glmnet <- glmnet(X_train, Y_train, family = "binomial",
                       alpha = 0.5, lambda = fit_en_cv$lambda.1se)

# use folds to make sure the predictions line up
  cvPred[tmp_test_idx] <- predict(fit_glmnet, newx = X_test, type = "response")
}
df2$cvPred <- cvPred

# Plot ROC
g_cv <- ggplot(df2, aes(d = diabetes, m = cvPred)) +
  geom_roc(n.cuts = 0) +
  style_roc() +
  ggtitle("10-Fold Cross-Validated ROC - Elastic Net")
print(g_cv)
calc_auc(g_cv)$AUC
```
I used 10-fold cross-validation to evaluate how well the Elastic Net model predicts diabetes. After training and testing the model across the folds, I plotted the ROC curve, which shows how well the model separates people with and without diabetes. The curve is close to a straight diagonal line, which means the model isn't doing much better than random guessing. The AUC (area under the curve) is 0.506, which is very close to 0.5—the score you'd expect from a model that’s just guessing. This suggests that the current set of health and demographic predictors may not be sufficient for accurately predicting diabetes, even though the modeling approach itself is statistically appropriate.

# 2. Using the random forest predictor you estimated, apply the permutation feature importance method.

```{r}
df <- read.csv("HW2_dataset.csv")

Y_glmnet <- df$Y
X <- df %>% select(-Y)

X_glmnet <- model.matrix(~ ., data = X)[, -1] 

str(Y_glmnet)
str(X_glmnet)
```

```{r}
set.seed(20)

# Define tuning parameters
p           <- ncol(X_glmnet)
mtry_rf     <- floor(p / 3)
ntree_rf    <- 500
nodesize_rf <- 5

# Create 10 random folds 
n <- nrow(X_glmnet)
folds <- sample(rep(1:10, length.out = n))

cv_mse <- numeric(10)

for (i in 1:10) {
  test_idx  <- which(folds == i)
  train_idx <- setdiff(1:n, test_idx)
  
  X_train <- X_glmnet[train_idx, , drop = FALSE]
  Y_train <- Y_glmnet[train_idx]
  X_test  <- X_glmnet[test_idx, , drop = FALSE]
  Y_test  <- Y_glmnet[test_idx]
  
  rf_model <- randomForest(
    x         = X_train,
    y         = Y_train,
    ntree     = ntree_rf,
    mtry      = mtry_rf,
    nodesize  = nodesize_rf
  )
  
  preds <- predict(rf_model, X_test)
  cv_mse[i] <- mean((Y_test - preds)^2)
}

mean_cv_mse <- mean(cv_mse)
cat("10-Fold CV MSE for Random Forest:", mean_cv_mse, "\n")
print(mean_cv_mse)
```

```{r}
# Permutation feature importance method
library(iml)

X_df <- as.data.frame(X_glmnet)

pred.rf <- Predictor$new(final_rf_model, data = X_df, y = Y_glmnet)

importance <- FeatureImp$new(pred.rf, loss = "mse")
plot(importance)

# Top Features
importance <- FeatureImp$new(pred.rf, loss = "mse") 
importance_df <- importance$results
head(importance_df[order(-importance_df$importance), ], 5)

# Accumulated local effects plots
top_features <- importance_df[order(-importance_df$importance), "feature"]
for (i in 1:3) {
  ale <- FeatureEffect$new(pred.rf, feature = top_features[i], method = "ale")
  print(plot(ale) + ggtitle(paste("ALE Plot for", top_features[i])))
}

```

## a. Which features have the greatest impact on the mean squared error loss function for the random forest predictor?

Based on the permutation feature importance output, the top 3 features that have the greatest impact on the mean squared error (MSE) loss function for the random forest predictor are:

X34 - Importance: 4.46, Permutation error: 92.16

X72 - Importance: 3.59, Permutation error: 74.15

X1 - Importance: 2.65, Permutation error: 54.82

These features caused the largest increase in MSE when permuted, indicating they are the most influential in the model's predictions.

## b. For the top features, do any exhibit non-linearities (e.g. using accumulated local effects plots)?

Yes, the ALE plots for the top three features X34, X72, and X1 demonstrate clear non-linear relationships. These non-linear patterns highlight the flexibility of the random forest model in capturing complex feature-outcome relationships that would be missed by linear models.

X34: The ALE plot for X34 shows a strong upward trend with a sharp inflection around the center of the distribution, suggesting that the feature has a substantial nonlinear impact on the prediction. The steep change indicates that small increases in X34 around this region significantly raise the predicted outcome.

X72: The ALE plot for X72 also exhibits a steep nonlinear pattern, with the effect on the outcome increasing rapidly in the middle range.

X1: The ALE plot for X1 reveals a smoother, S-shaped nonlinear relationship, indicating that its influence on the outcome varies across its range. The gradual slope changes show how X1 contributes differently depending on its value.
